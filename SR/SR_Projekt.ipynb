{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SR_Projekt_6_140683.ipynb","provenance":[],"collapsed_sections":["7ZllBBF6xe3t","IjIMrNWuhLa8","wMd_AcO6hIGK","AxnCmbs6lVwX","dRQTqTwdmweN","HHdVaVqGiEzr"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZyTCq4bzdr_g"},"source":["## **Aktywne uczenie: Zaawansowane aktywne uczenie z próbkowaniem na bazie puli, symulacja działań użytkownika i UI**"]},{"cell_type":"markdown","metadata":{"id":"7ZllBBF6xe3t"},"source":["### 1. Wprowadzenie. Praktyczne znaczenie aktywnego uczenia.\n","\n","**Uczenie maszynowe** odnosi się do szerokiego zakresu metod i algorytmów, które umożliwiają modelowi predykcyjnemu uczenie się zachowań za pomocą obserwacji. W praktyce zbieranie i oznaczanie przykładów szkoleniowych może być kosztownym zadaniem w przypadku wielu nadzorowanych problemów klasyfikacji. Koszt ten może wynikać z zapotrzebowania na eksperta, użycia sprzętu pomiarowego lub zaporowego czasu obliczeń. W wielu przypadkach mamy do czynienia z nieoznaczonymi danymi, tzn. takimi które nie posiadają etykiet, czyli przypisanych wartości. Danych oznaczonych jest niewiele. **Uczenie aktywne** to rodzaj uczenia maszynowego, który wykorzystuje się w przypadku, gdy mamy duży zbiór nieoznakowanych danych (jest to tzw. **pool-based sampling algorithm**, czyli algorytm wykorzystujący do uczenia się przykłady z puli danych). Algorytm wybiera najlepsze przykłady, które mają zostać oznakowane, bazując na metryce wartości uczącej (określa ona, który z przykładów może dać dla algorytmu najwięcej wiedzy) i wysyła zapytania do użytkownika będącego pewnego rodzaju wyrocznią, aby ten oznaczył nowe punkty danych prawdziwymi etykietami, które następnie trafiają z powrotem do algorytmu w celu ich przetworzenia, zanim następny przykład zostanie pokazany do rozważenia. W odróżnieniu od **uczenia pasywnego**, **uczenie aktywne** nie wymaga oznaczania całego zestawu danych. Model może działać lepiej ucząc się na mniejszej ilości danych, jeżeli może wybierać te dane, na których chce się uczyć. W ten sposób maksymalizowana jest wydajność. Niektóre przykłady dostarczają więcej informacji niż inne, ponieważ nie wszystkie są równie ważne. Najbardziej pouczające punkty danych to zazwyczej te, co do których model ma największe wątpliwości. Etykiety dla danych nieoznaczonych możemy najczęściej uzyskać pewnym kosztem. Aby tego dokonać możemy skorzystać z **crowdsourcingu**, czyli poprosić grupę ludzi o wybiórcze etykietowanie niektórych elementów zbioru danych. Minimalizujemy w ten sposób koszty pracy związane z etykietowaniem. Aktywne uczenie może przynieść korzyści w wielu praktycznych sytuacjach. Przykładowo informacje o preferencjach nowego użytkownika w systemie rekomendacji filmów można poprawić, starannie wybierając kilka filmów do oceny. Dane obejmujące oceny dla istniejących użytkowników mogą dostarczać strategię, która skutecznie kształtuje preferencje dla nowych użytkowników. Aktywne uczenie wykorzystywane jest również w **analizie obrazu**, **rozpoznawaniu mowy** oraz **tłumaczeniu maszynowym**.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1Kql7CiBX8f6BrArIfkDOg-0uBEPkcXwq)\n","\n","**Aktywne uczenie**, oparte na strumieniu, jest uważane za **częściowo nadzorowaną** metodę uczenia się. Poprzez iteracyjne zwiększanie rozmiaru naszego zestawu treningowego oznaczonego etykietą, możemy osiągnąć większą wydajność, niemal w pełni nadzorowaną, przy ułamku kosztów lub czasu szkolenia przy użyciu wszystkich danych. Trenujemy nasz model, aby zrównoważyć wydajność zadania i koszt etykietowania. Wszystkie strategie aktywnego uczenia się mają ten sam cel: oznaczenie jak najmniejszej liczby przykładów, przy ustalonych wynikach, jakie model predykcyjny musi osiągnąć.\n","\n","Więcej szczegółowych informacji na temat **aktywnego uczenia** znajdziemy tutaj:\n","\n","https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\n"]},{"cell_type":"markdown","metadata":{"id":"-TAEwyd9nYYA"},"source":["### 2. Ranking predykcji. \n","\n","#### **Jak otrzymujemy ranking predykcji oparty na modelu?**\n","\n","Przykładowo mamy dane przedstawione w postaci punktów w przestrzeni dwuwymiarowej. Dwa z nich są oznaczone, zaklasyfikowane: jeden należy do klasy czerwonych punktów, a drugi do klasy zielonych punktów. Czerwone, przerywane linie, które są wyznaczane na podstawie aktualnie niedostrojonej funkcji algorytmu, oznaczają **szacowaną granicę** między punktami przewidywanymi jako te należące do klasy czerwonych oraz te należące do klasy zielonych. \n","\n","![picture](https://drive.google.com/uc?export=view&id=1WMaggK23lrTA-383xG1kyx4i4q_U8JfM)\n","\n","Aby zwiększyć dokładność naszego modelu, wybieramy jeden z punktów, dla którego model jest **najbardziej niepewny**. Jest to punkt znajdujący się najwyżej w **rankingu predykcji** jako punkt o największej wartości niepewności. Najczęściej znajduje się na **granicy decyzyjnej** modelu. Punkt, który w tym przykładzie wybraliśmy został oznaczony poniżej niebieskim, przerywanym okręgiem.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1L90ADWgnET-38LTgl28mNPqJEB955xUz)\n","\n","Jest on wysyłany do **wyroczni**, która nakłada na niego etykietę, dzięki czemu mamy pewność do jakiej klasy zaliczany jest ów punkt. Następnie wykonywany jest proces uczenia i dostrajania algorytmu uwzględniający tą dodatkową daną. W ten sposób dostosowywana jest granica decyzji, a model dzięki temu będzie najprawdopodobniej działał lepiej. Dlaczego najprawdopodobniej? Ponieważ przy takich pojedynczych dostrojeniach jego dokładność może się wahać, ale w długiej perspektywie **trend dokładności predykcji będzie wzrostowy**.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1YaNs4awNmuJX8xG5M6XwRoOzAvAuzihE)\n","\n","Następnie wybieramy kolejny punkt znajdujący się najwyżej w rankingu i proces ten powtarzamy do momentu otrzymania wymaganej wydajności.\n","\n","Przy jednakowym, losowym próbkowaniu we wszystkich przykładach, wyuczony model nie do końca odzwierciedlałby podział na klasy. Jednak **aktywne uczenie** wybiera przykłady w pobliżu **granicy klasy** i jest w stanie znaleźć bardziej reprezentatywny klasyfikator."]},{"cell_type":"markdown","metadata":{"id":"C7WneGxXhQgF"},"source":["#### **Ranking predykcji w oparciu o wiele klas**\n","\n","Powyższy przykład składał się zaledwie z dwóch klas predykcyjnych. Poniżej przedstawiony zostanie bardziej rozbudowany sposób tworzenia rankingu w oparciu o **10 klas** (reprezentujących cyfry od **0** do **9**) z wykorzystaniem heurystyki. W tym przypadku model będzie uczony rozpoznawania cyfr na obrazkach.\n","\n","Wykonujemy predykcje dla przykładowych obrazków reprezentujących cyfry **5** i **4**:\n","\n","![picture](https://drive.google.com/uc?export=view&id=1krR7yCIQ4vEZQKMlKHxTBSWRrx8Uzbd2)\n","\n","Po wykonaniu poniższych linii kodu model wyznacza, która z cyfr według niego najbardziej odzwierciedla to co widzi na wskazanym obrazku. Możemy to sobie zwizualizować jako dwa wykresy prezentujące pewność modelu co do każdej z wymienionych cyfr na osi **X**. Im wyszy słupek przy, którejś z cyfr, tym algorytm jest bardziej przekonany co do niej.\n","\n","`model.predict(<powyższe_zdjęcie_cyfry_5>)`\n","\n","![picture](https://drive.google.com/uc?export=view&id=1l3UFfqHigK-4yKxAQmZNlx8psSvWzRQ2)\n","\n","`model.predict(<powyższe_zdjęcie_cyfry_4>)`\n","\n","![picture](https://drive.google.com/uc?export=view&id=11aGUOSaPqUbegt7hRP9LvGan8D-lm1ge)\n","\n","Jak widzimy z powyższych wykresów, w przypadku predykcji dla obrazka zawierającego cyfrę **5**, model kompletnie się nie sprawdza, ponieważ według niego prawdopodobieństwo, że jest to cyfra **5** równe jest **0**. Najbardziej prawdopodobna w tym przypadku jest dla modelu wartośc **6**, co nie jest prawdą. Występuje tu duża entropia jeżeli chodzi o rozłożenie prawdopodobieństw.\n","\n","Odwrotną sytuację mamy dla obrazka przedstawiającego cyfrę **4**. Tutaj model obrazuje stan wysokiej pewności i wyznacza prawidłowy wynik. Pewność dla cyfry **4** w tym przypadku jest drastycznie większa niż dla pozostałych wartości."]},{"cell_type":"markdown","metadata":{"id":"IjIMrNWuhLa8"},"source":["##### **Heurystyka marginesu**\n","\n","Aby uplasować oba przykłady w odpowiedniej kolejności w rankingu, możemy posłużyć się **heurystyką marginesu**. W tym przypadku liczymy różnicę pomiędzy wartością najbardziej prawdopodobnej klasy, a wartością drugiej najbardziej prawdopodobnej klasy w modelu. W przypadku **predykcji piątki** (różnica między **6** i **0**) otrzymujemy wartość **0.02**, a w przypadku **predykcji czwórki** (różnica między **4** i **2**) jest to **0.81**. Im niższa różnica w wartościach prawdopodobieństw, tym dany przykład plasuje się wyżej w rankingu.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1YhiuMoAkl_FSwdcZ5_EKUu2Wsku--7z6)"]},{"cell_type":"markdown","metadata":{"id":"wMd_AcO6hIGK"},"source":["##### **Heurystyka entropii**\n","\n","Podobnie jak w przypadku **heurystyki marginesu**, entropia jest zdecydowanie wyższa dla **predykcji piątki** niż dla **predykcji czwórki**, ponieważ w tej pierwszej wysokie prawdopodobieństwa o małych różnicach są rozłożone na wielu klasach, a ta druga przypisuje zdecydowaną większość prawdopodobieństwa jednej klasie. Wartości **entropii** podobnie jak **marginesu** możemy użyć do ustalenia rankingu predykcji wyznaczającego, które przykłady należy oznaczyć etykietą jako pierwsze.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1pS5bKdBBkWoVAWa2rLUH52PjazHaYuvA)"]},{"cell_type":"markdown","metadata":{"id":"AxnCmbs6lVwX"},"source":["### 3. Problem eksploracji-eksploatacji.\n","\n","Dylemat **eksploracji-eksploatacji** jest problemem, który można spotkać w większości procesów decyzyjnych opartych na danych, gdzie istnieje pewien rodzaj sprzężenia zwrotnego między zbieraniem danych a podejmowaniem decyzji. Występuje on w przypadku, gdy model jest stale aktualizowany, a jego trenowanie odbywa się w tym samym czasie, w którym zbierane są dane. Wybór danych w nieoznaczonym zbiorze w przestrzeni **X** jest określany jako **eksploracja danych**. Oznacza to próbkowanie tej przestrzeni tak, aby potencjalnie zminimalizować błędy predykcji przez uczący się model. Strategia zakładająca wyłączną **eksploatację danych**, ignoruje sporą część całego obszaru danych, a model trenowany jest tylko na określonym obszarze **X**, przez co nie będzie równie efektywny w kontekście całej przestrzeni. Z kolei model skupiający się wyłącznie na **eksploracji danych**, będzie pomijał dane z przestrzeni **X**, których oznaczenie mogłoby znacznie podnieść jego skuteczność. Kompromis między **eksploracją** a **eksploatacją** musi być dokładnie rozważony. Polega on na zadaniu sobie pytania: czy powiniśmy wybrać to, co znamy i uzyskać w ten sposób coś zbliżonego do tego, czego oczekujemy, czy wybrać coś, czego nie jesteśmy pewni i ewentualnie dowiedzieć się więcej? Ciągłe etykietowanie przykładów szkoleniowych jest kosztownym zadaniem w nadzorowanym uczeniu się. Wybór przykładów do oznaczenia można uznać za dylemat między eksploracją a eksploatacją przestrzeni danych wejściowych. Strategie **aktywnego uczenia** rozwiązują ten problem i wybierają nieoznaczone przykłady, które są uważane za najbardziej przydatne do uczenia modelu predykcyjnego, na podstawie wcześniej wspomnianego rankingu. "]},{"cell_type":"markdown","metadata":{"id":"dRQTqTwdmweN"},"source":["### 4. Wpływ możliwości UI na wydajność aktywnego uczenia.\n","\n","Najczęściej chcemy, aby nasz projekt opierający się na uczeniu aktywnym był skalowalny. Chcemy, aby wielu użytkowników mogło nakładać etykiety na przykładowe dane jednocześnie. W tym celu możemy udostępnić dane na serwerze, które będą dysponować tym samym rankingiem wszystkich nieoznaczonych danych dla wszystkich etykietujących. Proces uczenia niekoniecznie musi się wykonywać za każdym podesłanym **requestem** przez użytkownika zawierającym poprawną etykietę. Serwer może zostać skonfigurowany w ten sposób, aby co jakiś określony czas lub co określoną liczbę dodanych etykiet, uruchamiany był proces uczenia się, sprawiający, że model będzie stawał się coraz dokładniejszy. Niezbędny w tym działaniu jest **User Interface**, który zapewnia tzw. wyroczniom wygodny sposób etykietowania danych. Korzystając z wielu rozproszonych jednostek zaopatrzonych w **UI** proces uczenia staje się znacznie bardziej efektywny, niż gdyby miała robić to pojedyncza osoba."]},{"cell_type":"markdown","metadata":{"id":"rDIzrgxLXhch"},"source":["### 5. Implementacja aktywnego uczenia, bazująca na modAL.\n","\n","W tym punkcie zostaną przedstawione dwa przykłady aplikacji bazujących na **aktywnym uczeniu**, zbudowanych w oparciu o framework **modAL**. Jedna z nich zostanie zaopatrzona w **GUI**, a druga będzie stanowić symulację działań użytkownika etykietującego przykłady i automatycznego uczenia algorytmu.\n","\n","#### **Instalacja biblioteki modAL**\n","**modAL** to framework aktywnego uczenia bazujący na **Python3**, zaprojektowany z myślą o modułowości, elastyczności i rozszerzalności. Zbudowany na bazie **scikit-learn**, umożliwia szybkie tworzenie mechanizmów aktywnego uczenia."]},{"cell_type":"code","metadata":{"id":"0b9MEg8oOwT7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643290815475,"user_tz":-60,"elapsed":4992,"user":{"displayName":"Maciej Chajda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHDOZVgnow41uvYqaY0-rzwAjCNx1xf6FYQhlSPw=s64","userId":"17515786993221931389"}},"outputId":"99704665-1256-4396-d52f-e144e765e094"},"source":["pip install modAL"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting modAL\n","  Downloading modAL-0.4.1-py3-none-any.whl (27 kB)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.1.5)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.0.2)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.4.1)\n","Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (3.0.0)\n","Installing collected packages: modAL\n","Successfully installed modAL-0.4.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"LgEZBplhMHsw"},"source":["#### **Importujemy wymagane biblioteki**\n","\n","Część z poniżych bilibotek odpowiada bezpośrednio za uczenie maszynowe, ale niektóre z nich zostały również zaimportowane w celu utworzenia **GUI**. Pokrótce omówimy do czego je wykorzystujemy:\n","*   **modAL.uncertainty** - importujemy funkcje *margin_sampling()*, *uncertainty_sampling()* oraz *entropy_sampling()*, które wykorzystamy w poniższych przykładach jako strategie zapytań w klasie *ActiveLearner*.\n","*   **sklearn.model_selection** - korzystamy z funkcji *train_test_split()* w celu podzielenia zbioru danych pobranych z *load_digits()*, na *dane treningowe* oraz *dane testowe*.\n","*   **sklearn.ensemble** - importujemy estymator *RandomForestClassifier*, który wykorzystamy w klasie *ActiveLearner*.\n","*   **sklearn.datasets** - korzystamy z funkcji *load_digits()*, która importuje nam zbiór obrazków przedstawiających cyfry oraz ich prawidłowe etykiety, na postawie których, w jednym z poniższych przykładów, będziemy uczyć nasz algorytm.\n","*   **modAL.models** - importujemy klasę *ActiveLearner*, która stanowi podstawę naszego algorytmu aktywnego uczenia. To w niej deklarujemy poszczególne parametry, tj. *estymator*, czy *strategię zapytań*.\n","*   **google.colab** - wykorzystujemy moduł *output*, aby wywołać w odpowiednim momencie funkcję *.clear()* w celu odświeżenia okna.\n","*   **matplotlib.pyplot** - wykorzystujemy do wyświetlenia obrazków przedstawiających cyfry, które pochodzą ze zbioru *load_digits().data* oraz do wyświetlenia wyników w formie wykresów.\n","*   **ipywidgets** - wkorzystujemy do utworzenia *GUI*, a dokładniej do wyrysowania przycisków pozwalających wybrać użytkownikowi cyfrę, którą widzi w danym momencie na obrazku.\n","*   **numpy** - pozwala na obsługę dużych, wielowymiarowych tabel i macierzy oraz na wykonywanie działań na nich.\n","*   **functools** - moduł pozwalający przekazać funkcji *on_button_click()* dodatkowe argumenty.\n","\n","Dodatkowe informacje na temat działania **Random Forst** możemy znajeźć tutaj:\n","https://en.wikipedia.org/wiki/Random_forest"]},{"cell_type":"code","metadata":{"id":"m-mf7LBcKZrs","executionInfo":{"status":"ok","timestamp":1643290829344,"user_tz":-60,"elapsed":1614,"user":{"displayName":"Maciej Chajda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHDOZVgnow41uvYqaY0-rzwAjCNx1xf6FYQhlSPw=s64","userId":"17515786993221931389"}}},"source":["from modAL.uncertainty import margin_sampling, entropy_sampling, uncertainty_sampling\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_digits\n","from modAL.models import ActiveLearner\n","from google.colab import output\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","import numpy as np\n","import functools "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3pWn9TqDKOex"},"source":["#### **Tworzymy obiekt ActiveLearner**\n","\n","Aby utworzyć obiekt **ActiveLearner**, musimy podać funkcję strategii zapytań (domyślną strategią jest **strategia próbkowania maksymalnej niepewności**). Możemy wykorzystać wbudowane funkcje strategii zapytań w **modAL.uncertainty** lub zaimplementować własne.\n","\n","**modAL.uncertainty** dysponuje następującymi funkcjami:\n","*   **uncertainty_sampling** - wybiera najmniej pewną instancję do etykietowania.\n","*   **margin_sampling** - wybiera instancje, w których różnica między pierwszą najbardziej prawdopodobną a drugą najbardziej prawdopodobną klasą jest najmniejsza.\n","*   **entropy_sampling** - wybiera instancje, w których prawdopodobieństwa klas mają największą entropię.\n","\n","W tym przypadku skorzystamy z **margin_sampling**. Wczytujemy dane, które będziemy wykorzystywać do uczenia i umieszczamy je w zmiennej **digits**. Następnie za pomocą funkcji **train_test_split()** tasujemy dane na **dane treningowe** i **dane testowe** w proporcji **1:4** (ponieważ **test_size=0.8**). W **X_init** i **y_init** umieszczamy dane inicjujące. Jest to zbiór dziesięciu pierwszych przykładów z **digits.data**. Wprowadzamy je następnie do pól **X_training** i **y_training** w **ActiveLearner**."]},{"cell_type":"code","metadata":{"id":"T6JGbjomMcQg","executionInfo":{"status":"ok","timestamp":1643290890621,"user_tz":-60,"elapsed":1000,"user":{"displayName":"Maciej Chajda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHDOZVgnow41uvYqaY0-rzwAjCNx1xf6FYQhlSPw=s64","userId":"17515786993221931389"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa51b35f-305d-4f84-9363-5b048458405c"},"source":["digits = load_digits()\n","X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.8)\n","\n","X_init = digits.data[0:10]\n","y_init = digits.target[0:10]\n","\n","regressor = ActiveLearner(\n","    estimator=RandomForestClassifier(),\n","    query_strategy=margin_sampling,\n","    X_training=X_init, y_training=y_init\n",")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n","       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n","       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n","       ...,\n","       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n","       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n","       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'frame': None, 'feature_names': ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7'], 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n","        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n","        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n","        ...,\n","        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n","        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n","        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n","\n","       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n","        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n","        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n","        ...,\n","        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n","        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n","        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n","\n","       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n","        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n","        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n","        ...,\n","        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n","        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n","        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n","\n","       ...,\n","\n","       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n","        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n","        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n","        ...,\n","        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n","        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n","        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n","\n","       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n","        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n","        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n","        ...,\n","        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n","        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n","        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n","\n","       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n","        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n","        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n","        ...,\n","        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n","        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n","        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}\n"]}]},{"cell_type":"markdown","metadata":{"id":"FBGQvtAsQa8_"},"source":["#### **Definiowanie funkcji odpowiedzialnych za uczenie i GUI**\n","\n","*   **run()** - w pierwszej kolejności za pomocą funkcji *.query()* pobieramy *query_idx*, czyli indeks instancji ze zbioru *X_train*, która znajduje się najwyżej w rankingu predykcji. W kolejnej linii kodu stosujemy funkcję *.imshow()* do wyrysowania obrazka przedstawiającego cyfrę, który znajduje się na *query_idx* pozycji w zbiorze *X_train*. *print()* dla wygody podpowiada nam jaka cyfra znajduje się na obrazku. Następnie w pętli *for* rysujemy dziesięć przycisków zapopatrzonych w wartości od *0* do *9*. Każdy z przycisków ma przypisaną funkcję *on_button_click()*, która wywoaływana jest po ich naciśnięciu.\n","*   **on_button_click()** - jako argument funkcja ta przyjmuje obiekt *i*, który zaopatrzony jest w pole *k* określające wartość przycisku, który został kliknięty oraz pole *query_idx* mówiące o indeksie instancji ze zbioru *X_train*, która znajduje się najwyżej w rankingu predykcji. Na początku za pomocą funkcji *.clear()* czyszczone jest przestarzałe okno wyświetlające *GUI*. W celu jego ponownego wyrysowania, już z nowym przykładowym obrazkiem, wykonywana jest funkcja *run()*. Korzystając z funkcji *.teach()* uczymy algorytm, przypisując przykładowi znajdującemu się na pozycji *query_idx*, wartość *k* wybraną przez użytkownika. Na koniec *print()* przedstawia nam wartość funkcji *.score()*, która zostanie omówiona poniżej.\n","\n"]},{"cell_type":"code","metadata":{"id":"hVpkzLkVuafH","executionInfo":{"status":"ok","timestamp":1643290808535,"user_tz":-60,"elapsed":968,"user":{"displayName":"Maciej Chajda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhHDOZVgnow41uvYqaY0-rzwAjCNx1xf6FYQhlSPw=s64","userId":"17515786993221931389"}}},"source":["def run():\n","  query_idx, _ = regressor.query(X_train)\n","  plt.imshow(X_train[query_idx, :].reshape(8, 8), cmap='Greys_r')\n","  print(\"correct:\", y_train[query_idx])\n","  for k in range(10):\n","      button = widgets.Button(description=str(k), button_style='info')\n","      display(button)\n","      button.on_click(functools.partial(on_button_click, i={\"k\":k,  \"query_idx\":query_idx}))\n","\n","def on_button_click(_, i):\n","  output.clear()\n","  regressor.teach(X_train[i['query_idx']], [i['k']])\n","  print(\"score: \", regressor.score(X_test, y_test))\n","  run()"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHdVaVqGiEzr"},"source":["#### **Pomiar wydajności**\n","\n","W celu ustalenia jak wydajny jest nasz algorytm i jakie postępy poczyniliśmy ucząc go, korzystamy z funkcji wbudowanej **.score()** obiektu **ActiveLearner**. Przyjmuje ona dwa arugmenty: **dane testowe X** oraz ich **prawidłowe etykiety y**. Funkcja poddaje predykcji kolejne dane ze zbioru **X** oraz porównuje wyniki z tymi ze zbioru **y**. Jako wynik wyjściowy (gdy korzystamy z estymatora **RandomForestClassifier()**) otrzymujemy wartość z przedziału od **0.00** do **1.00**, która mówi nam o poziomie wyszkolenia alogrytmu (wartość **1.00** to najlepszy wynik, który oznacza, że algorytm prawidłowo zdiagnozował **100%** podanych instancji. Innymi słowy przypisał prawidłowe etykiety, równe tym, które otrzymał od wyroczni, wszystkim nieetykietowanym elementom **zbioru X**)."]},{"cell_type":"markdown","metadata":{"id":"WQnQdTB8UqlX"},"source":["#### **Uruchomienie aplikacji**\n","\n","Aplikację uruchamiamy za pomocą poniższej funkcji. Po jej wywołaniu użytkownik musi wybierać cyfry, które widzi na obrazku, poprzez klikanie przycików im odpowiadającym, do momentu uzyskania satysfakcjonującego wyniku **score**."]},{"cell_type":"code","metadata":{"id":"g2t1jtHSQ0Dn"},"source":["run()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"92UfiDw7mmAr"},"source":["#### **Symulacja etykietowania i aktywnego uczenia**\n","W tym podpunkcie przedstawiony został kod, który pomija interfejs użytkownika. Zamiast wproawdzać dane ręcznie, dysponujemy gotowym zbiorem etykiet. Działanie kodu wygląda tak samo, jakby robił to ręcznie użytkwnik, ale tutaj chcemy się skupić na szybkiej symulacji wyników działania.\n","\n","Wczytujemy bazę obrazków zawierających cyfry oraz ich prawidłowe etykiety do zmiennej **digits**. Korzystamy z funkcji **train_test_split()** w celu wygodnego podzielenia zbioru danych na dane treningowe (**X_train** i **y_train**) oraz dane testowe (**X_test** i **y_test**) w proporcji **1:4**. Tworzymy obiekt **ActiveLearner**, któremu przypisujemy estymator **RandomForestClassifier()** i ustawiamy tym razem strategię zapytań **entropy_sampling**, aby pokazać, że ona również działa i nadajemy dane początkowe w polach **X_training** i **y_training**.\n","\n","W tablicy **scores** będziemy przechowywać kolejne wyniki funkcji **.score()** pochodzące z każdej iteracji pętli **for** uczącej algorytm. \n","\n","W samej już pętli, funkcja **.query()** zwraca nam **query_index**, który określa, na której pozycji w tablicy **X_train** znajduje się instancja o największej wartości uczącej dla algorytmu (instancja, która znajduje się najwyżej w rankingu predykcji). W tym przypadku użytkownika, który jest nauczycielem, zastępuje zbiór poprawnych odpowiedzi **y_train**, który w funkcji **.teach()** przypisuje poprawną etykietę dla danej instancji **X_train[[i]]**. Co dziesiątą instancję wyświetlamy metrykę wydajności obliczoną za pomocą funkcji **.score()**, dla której wprowadzamy dane testowe **X_test** i **y_test**."]},{"cell_type":"code","metadata":{"id":"CzH1eguEn25R"},"source":["digits = load_digits()\n","X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.8)\n","\n","X_init = digits.data[0:10]\n","y_init = digits.target[0:10]\n","\n","scores = []\n","\n","learner = ActiveLearner(\n","    estimator=RandomForestClassifier(),\n","    query_strategy=entropy_sampling,\n","    X_training=X_init, y_training=y_init\n",")\n","\n","print('All training instances: ', len(X_train))\n","\n","for i in range(200):\n","  query_idx, query_inst = learner.query(X_train)\n","  learner.teach(X_train[query_idx], y_train[query_idx])\n","  score = learner.score(X_test, y_test)\n","  scores.append(score)\n","  if i % 10 == 0:\n","    print(\"score (\", i, \"instances ): \", score)\n","print(\"score ( final ): \", learner.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dm3k1a29fAbV"},"source":["Aby zobrazować proces uczenia, możemy uruchomić poniższy kod, który wyświetla w formie wykresu poziom wyuczenia algorytmu z każdej iteracji pętli. Na osi **Y** znadują się wartości w przedziale **<0 ; 1>**, opisujące efektywność algorytmu, a na osi **X** znajduje się liczba wykonanych iteracji. Jak widać, największy skok obserwujemy pomiędzy **pierwszą** iteracją a **pięćdziesiątą**. Następnie wraz z kolejnymi iteracjami, wzrost poziomu wyuczenia algorytmu znacząco zwalnia, mimo wszystko zachowując nadal **trend wzrostowy**. "]},{"cell_type":"code","metadata":{"id":"HsacQqXZdL5C"},"source":["y = np.array(scores)\n","X = np.arange(len(scores))\n","plt.plot(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezQftMSnwuwP"},"source":["#### **Porównanie strategii**\n","Na zakończenie tego punktu sprawdzimy, jak obrazują się względem siebie na wykresie, trzy wymienione strategie: **uncertainty_sampling**, **margin_sampling** oraz **entropy_sampling**. Każda z nich działa inaczej, ale mają ten sam cel, czyli wyznaczenie wartości uczącej dla każdego przykładu i wybranie na tej podstawie najlepszej instancji. Strategie te opierają się na niepewności klasyfikacji, stąd nazywane są miarami niepewności.\n","\n","Szczegółowe informacje dotyczące działania każdej z tych strategii, możemy znaleźć na poniższej stronie:\n","\n","https://modal-python.readthedocs.io/en/latest/content/query_strategies/uncertainty_sampling.html\n","\n","Aby zebrać szczegółowe dane na temat wyników efektywności algorytmu dla każdej strategii, wykonujemy poniższy kod.\n","\n","Tak samo jak wcześniej, wczytujemy bazę obrazków zawierających cyfry oraz ich prawidłowe etykiety do zmiennej **digits**. Korzystamy z funkcji **train_test_split()** w celu podzielenia zbioru danych na **dane treningowe** oraz **dane testowe** w proporcji **1:4**. W tablicy **scores_matrix** będziemy przechowywać kolejne tablice wyników funkcji **.score()** pochodzące z każdej iteracji pętli **for** uczącej algorytm, dla poszczególnych strategii. W tablicy **strategies** przechowujemy wszystkie trzy wcześniej wymienione strategie, a w **strategies_names** ich nazwy.\n","\n","W pętli **for** tworzymy obiekt **ActiveLearner**, któremu przypisujemy estymator **RandomForestClassifier()** i strategię zapytań, wyjętą z tablicy **strategies**, w zależności od iteracji **i**. W tablicy **scores** będziemy przechowywać metryki efektywności algorytmu obliczone w każdej iteracji wewnętrznej pętli za pomocą funkcji **.score()**. \n","\n","W wewnętrznej pętli funkcja **.query()** zwraca nam **query_index**, który określa, na której pozycji w tablicy **X_train** znajduje się instancja o największej wartości uczącej dla algorytmu. Następnie w funkcji **.teach()** przypisuje poprawną etykietę dla tej instancji. \n","\n","Po każdym wykonaniu wewnętrznej 100-iteracyjnej pętli, wyświetlony zostanie jej wynik końcowy dla poszczególnych strategii, a tablica **scores** zostanie dodana do tablicy **scores_matrix**."]},{"cell_type":"code","metadata":{"id":"axr82NB-xrMY"},"source":["digits = load_digits()\n","X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.8)\n","\n","X_init = digits.data[0:10]\n","y_init = digits.target[0:10]\n","\n","scores_matrix = []\n","\n","strategies = [entropy_sampling, margin_sampling, uncertainty_sampling]\n","strategies_names = ['entropy_sampling', 'margin_sampling', 'uncertainty_sampling']\n","\n","for i in range(3):\n","  learner = ActiveLearner(\n","    estimator=RandomForestClassifier(),\n","    query_strategy=strategies[i],\n","    X_training=X_init, y_training=y_init\n","  )\n","  scores = []\n","  for _ in range(100):\n","    query_idx, query_inst = learner.query(X_train)\n","    learner.teach(X_train[query_idx], y_train[query_idx])\n","    score = learner.score(X_test, y_test)\n","    scores.append(score)\n","  print(f\"score ( {strategies_names[i]} ): \", learner.score(X_test, y_test))\n","  scores_matrix.append(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QE1x9GtkA_7N"},"source":["Aby wyświetlić zgromadzone wyniki w tablicy **scores_matrix** korzystamy z zaimportowanej biblioteki **pyplot** i wykonujemy poniższy kod. Jak widać po wykresie, szybkość uczenia dla każdej strategii jest bardzo zbliżona i nie można jednoznacznie ustalić, która z nich jest najlepsza."]},{"cell_type":"code","metadata":{"id":"eOIrSr3H34Fk"},"source":["x = np.arange(100)\n","\n","plt.plot(x, scores_matrix[0], 'r') #entropy_sampling\n","plt.plot(x, scores_matrix[1], 'g') #margin_sampling\n","plt.plot(x, scores_matrix[2], 'b') #uncertainty_sampling\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8l4BNt-W1ZZY"},"source":["### 6. Wykorzystanie Google Sheets i bazy The Boston Housing Dataset."]},{"cell_type":"markdown","metadata":{"id":"nGneiP7ZRj1n"},"source":["W poniższym przykładzie wykorzystamy dane pochodzące z bazy **The Boston Housing Dataset** w celu wytrenowania modelu predykcyjnego bazującego na **regresji**, aby ten mógł szacować cenę mieszkaniń na podstawie wprowadzonych danych.\n","\n","Plik, który będziemy wykorzystywać możemy pobrać ze strony:\n","\n","https://docs.google.com/spreadsheets/d/1G5oHZkaM8ztH1ftrHTHaF2BGXIplcJHBAxbCETiPJI4/edit?usp=sharing\n","\n","Znajduje się na niej plik **.xlsx** zawierający dane pochodzące z informacji zebranych przez **U.S. Census Service** dotyczących mieszkalnictwa na obszarze **Boston MA**. Poniżej opisane zostały kolumny zestawu danych:\n","\n","*   **CRIM** - wskaźnik przestępczości na mieszkańca według miast.\n","*   **ZN** - udział działek mieszkaniowych powyżej 25.000 stóp kwadratowych.\n","*   **INDUS** - odsetek akrów działalności niedetalicznej w mieście.\n","*   **CHAS** - zmienna określająca położenie budynku przy rzece Charles River (1, jeśli leży przy rzece; 0 w przeciwnym razie).\n","*   **NOX** - stężenie tlenków azotu (na 10 mln).\n","*   **RM** - średnia liczba pokoi na mieszkanie.\n","*   **AGE** - odsetek domów wybudowanych przed 1940.\n","*   **DIS** - ważone odległości do pięciu głównych bostońskich centrów zatrudnienia.\n","*   **RAD** - wskaźnik dostępności do autostrad międzystanowych.\n","*   **TAX** - pełnowartościowa stawka podatku od nieruchomości (za 10 000 USD).\n","*   **PTRATIO** - stosunek uczniów do nauczycieli według miasta.\n","*   **B** - 1000(Bk - 0,63)^2 gdzie Bk to odsetek czarnych według miasta.\n","*   **LSTAT** - procent ludność klasy niższej.\n","*   **MEDV** - mediana wartości domów zajmowanych przez właścicieli (w 1000 USD).\n","\n","Pobrany plik umieszczamy na dysku **Google Drive** w **rootowym** folderze, w formie pliku **.xlsx** i pozostawiamy jego domyślną nazwę pliku **The_Boston_Housing_Dataset** oraz domyślną nazwę arkusza, czyli **Arkusz1** (**Sheet1**)."]},{"cell_type":"markdown","metadata":{"id":"fvHyoWeNVl3q"},"source":["#### **Importujemy wymagane biblioteki**\n","Część z poniżych bilibotek odpowiada bezpośrednio za uczenie maszynowe, ale niektóre z nich zostały również zaimportowane w celu komunikacji z **Google Sheets**. Pokrótce je omówimy (pomijamy te, które zostały już omówione wcześniej):\n","*   **sklearn.gaussian_process.kernels** - importujemy elementy *DotProduct* oraz *WhiteKernel*, których głównym zastosowaniem będzie obliczenie kowariancji etymatora *GP* między punktami danych.\n","*   **sklearn.gaussian_process** - importujemy estymator *GaussianProcessRegressor*, który wykorzystamy w klasie *ActiveLearner*.\n","*   **oauth2client.client** - importujemy klasę *GoogleCredential* dla protokołu *OAuth 2.0* umożliwiającą dostęp do chronionych zasobów przy użyciu tokena dostępu.\n","*   **sklearn** - importujemy pakiet *preprocessing*, który udostępnia kilka typowych funkcji użytkowych i klas transformatorów do zmiany surowych wektorów cech w reprezentację, która jest bardziej odpowiednia dla estymatorów.\n","*   **google.colab** - importujemy moduł *auth*, który pozwala zidentyfikować użytkownika za pomocą wygenerowanego dla niego tokena.\n","*   **pandas** - szybkie, wydajne, elastyczne i łatwe w użyciu narzędzie do analizy i zarządzania danymi.\n","*   **gspread** - za pomocą tego modułu tworzymy obiekt, za pośrednictwem którego będziemy wykonywać różne operacje (otwieranie, aktualizowanie *Arkuszy Google* itp.) na naszym *Dysku Google*."]},{"cell_type":"code","metadata":{"id":"pSg-YgFi1YOF"},"source":["from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.model_selection import train_test_split\n","from oauth2client.client import GoogleCredentials\n","from modAL.models import ActiveLearner\n","from sklearn import preprocessing\n","from google.colab import auth\n","import pandas as pd\n","import numpy as np\n","import gspread"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lHRpbd5kpcaJ"},"source":["#### **Uwierzytelnianie**\n","\n","Uwierzytelniamy się wprowadzając w wyznaczone pole tekstowe **token**, który wygenerowany zostanie dla nas za pomocą funkcji **.authenticate_user()**. Dzięki temu nawiążemy kontakt z naszym **Dyskiem Google**, który wykorzystamy w następnych krokach."]},{"cell_type":"code","metadata":{"id":"FN-OdXSay_vA"},"source":["auth.authenticate_user()\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lg7IQRHWVhl1"},"source":["#### **Odczytanie zawartości pliku**\n","\n","Za pomocą poniższego kodu odczytujemy całą zawartość wcześniej dodanego pliku **The_Boston_Housing_Dataset**. Wykorzystujemy funkcję **.open()**, aby otworzyć wspomniany plik, a następnie poszczególne tablice wczytanych danych przypisujemy do zmiennych, których nazwy pokrywają się z nazwami powyżej opisanych kolumn. Nie będziemy wykorzystywać jednak wszystkich wierszy do trenowania modelu, ponieważ skorzystamy w tym przykładzie z estymatora **GaussianProcessRegressor**, który charakteryzuje się wysoką skutecznością, ale również jest kosztowny obliczeniowo dla dużych zbiorów danych. Wczytujemy pierwsze **305** z **507** wierszy. W zmiennej **parameters** umieszczamy **numpy.array** zawierającą kolumny, które wykorzystamy w trenowaniu algorytmu. Za pomocą **StandardScaler()** dokonujemy wstępnego przetworzenia danych zawartych w **parameters** i zapisujemy je w zmiennej **X**. Zmienna **y** przyjęła dane określające medianę cen mieszkań dla poszczególnych danych **X**."]},{"cell_type":"code","metadata":{"id":"RJ25zbKU1iyB"},"source":["worksheet = gc.open('The_Boston_Housing_Dataset').sheet1\n","\n","CRIM = [float(i.value) for i in worksheet.range('A2:A307')]\n","ZN = [float(i.value) for i in worksheet.range('B2:B307')]\n","INDUS = [float(i.value) for i in worksheet.range('C2:C307')]\n","CHAS = [float(i.value) for i in worksheet.range('D2:D307')]\n","NOX = [float(i.value) for i in worksheet.range('E2:E307')]\n","RM = [float(i.value) for i in worksheet.range('F2:F307')]\n","AGE = [float(i.value) for i in worksheet.range('G2:G307')]\n","DIS = [float(i.value) for i in worksheet.range('H2:H307')]\n","RAD = [float(i.value) for i in worksheet.range('I2:I307')]\n","TAX = [float(i.value) for i in worksheet.range('J2:J307')]\n","PTRATIO = [float(i.value) for i in worksheet.range('K2:K307')]\n","B = [float(i.value) for i in worksheet.range('L2:L307')]\n","LSTAT = [float(i.value) for i in worksheet.range('M2:M307')]\n","MEDV = [float(i.value) for i in worksheet.range('N2:N307')]\n","\n","parameters = np.array([CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT])\n","standard_scaler_fit = preprocessing.StandardScaler().fit(parameters)\n","\n","X = standard_scaler_fit.transform(parameters).transpose()\n","y = np.array(MEDV)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoiAlZ6IaoDY"},"source":["#### **Definiowanie regresora**\n","\n","Tworzymy obiekt **ActiveLearner**, któremu przypisujemy estymator **GaussianProcessRegressor()** i ustawiamy własną, zdefiniowaną strategię zapytań **GP_regression_std()**, która bierze pod uwagę punkty o największym odchyleniu standardowym. W tym przypadku nie uwzględniamy danych początkowych **X_training** i **y_training**."]},{"cell_type":"code","metadata":{"id":"iQiuYYwc3Ep_"},"source":["def GP_regression_std(regressor, X):\n","    _, std = regressor.predict(X, return_std=True)\n","    query_idx = np.argmax(std)\n","    return query_idx, X[query_idx]\n","\n","kernel = DotProduct() + WhiteKernel()\n","\n","regressor = ActiveLearner(\n","    estimator=GaussianProcessRegressor(kernel=kernel),\n","    query_strategy=GP_regression_std\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idAmI3OIgcvF"},"source":["#### **Trenowanie modelu**\n","\n","Korzystamy z funkcji **train_test_split()** w celu podzielenia zbioru danych na dane treningowe (**X_train** i **y_train**) oraz dane testowe (**X_test** i **y_test**) w proporcji **2:1**.\n","\n","Następnie definiujemy trzy zmienne:\n","*   **score** - określająca aktualny wynik wyuczenia algorytmu. Jako domyślną wartość podaliśmy liczbę **1**, aby dalej zdefiniowana pętla **while** mogła się wykonać.\n","*   **target** - określa bezwzględną wartość miary **score**, którą oczekujemy osiągnąć.\n","*   **i** - numer aktualnej iteracji.\n","\n","Ze względu na to, że korzystamy teraz z **GaussianProcessRegressor()** to funkcja **.score()** może dawać również ujemne wyniki, a najwyższy poziom wytrenowania ma wartość **0**. Dlatego też pętla **while** została zdefiniowana tak, aby wykonywała zawarty w niej  proces uczenia do momentu aż funkcja **.score()** zwróci wynik z przedziału **<-0.01 ; 0.01>**. "]},{"cell_type":"code","metadata":{"id":"1xWzK5dAdCFc"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n","\n","score = 1\n","target = 0.01\n","i = 0\n","\n","while score <= -target or score >= target:\n","  query_idx, query_inst = regressor.query(X)\n","  regressor.teach([query_inst], [y[query_idx]])\n","  score = regressor.score(X_test,y_test)\n","  i+=1\n","  if i % 10 == 0:\n","    print(\"score (\", i, \"instances ): \", score)\n","print(\"score (final): \", score)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tO64SxKKxICw"},"source":["#### **Średni błąd**\n","\n","Na zakończenie możemy z ciekawości sprawdzić jak wygląda średni błąd ceny predykcyjnej względem ceny prawdziwej dla wsztstkich wierszy danych zawartych w pliku **The_Boston_Housing_Dataset**. W tablicy **MEDV2** zapisujemy wyniki predykcyjne naszego wytrenowanego modelu, dla każdego wiersza, następnie w drugiej pętli **for** obliczamy sumę błędów, którą umieszczamy w zmiennej **sum**. Na koniec zmienną **sum** dzielimy przez liczbę **len(y_test)** (liczba wierszy) i zaokrąglamy do drugiego miejsca po przecinku."]},{"cell_type":"code","metadata":{"id":"7eDFhjnVswpM"},"source":["MEDV2 = []\n","\n","for i in X_test:\n","   MEDV2.append(regressor.predict([i]))\n","\n","sum = 0\n","\n","for i in range(len(y_test)):\n","   sum += abs((y_test[i]/MEDV2[i]) - 1)\n","\n","print(round(sum[0]/len(y_test), 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcXN3_WR5aSk"},"source":["### 7. Wykorzystanie Google Sheets jako GUI.\n","\n","W poniższym przykładzie wykorzystamy dane pochodzące z bazy **Mall_Customers** określające cechy klientów centrum handlowego (tj. **wiek**, **płeć** oraz **dochód roczny**), w celu wytrenowania modelu predykcyjnego bazującego na **regresji**, aby ten mógł szacować ich **Spending Score**. W tym celu wykorzystamy **Arkusz Google** jako **GUI** pozwalające użytkownikowi w łatwy sposób nakładać etykiety na dane.\n","\n","Poniżej opisane zostały kolumny zestawu danych:\n","\n","*   **GENRE** - płeć klienta.\n","*   **AGE** - wiek klienta.\n","*   **ANNUAL_INCOME** - dochód roczny klienta.\n","*   **SPENDING_SCORE** - miara mówiąca o podatności klienta na wydatki. "]},{"cell_type":"markdown","metadata":{"id":"6OE6k2VrPcSA"},"source":["####**Pobranie niezbędnych dokumentów**\n","\n","Poniższe pliki należy pobrać i umieścić w **rootowym** folderze na **Google Drive**:\n","\n","**Learning**\n","https://docs.google.com/spreadsheets/d/1oCFQFsCgdjXbxTpDrS4N9NleN3JOpquYgfeU3YuxaP8/edit?usp=sharing\n","\n","**Mall_Customers**\n","https://docs.google.com/spreadsheets/d/10pWYMvUCZQlaP_-0ONVD0L-mV-hgB2NAzBPCCQi3Nbo/edit?usp=sharing\n","\n","Nazwy plików oraz arkuszy powinny pozostać bez zmian."]},{"cell_type":"markdown","metadata":{"id":"h7IJXtdGi2r9"},"source":["####**Instalacja biblioteki gspread_formatting**\n","Biblioteka **gspread_formatting** wykorzystana zostanie w celu narysowania w **Arkuszu Google** elementów interfejsu takich jak **checkboxy** oraz kolorowania tła poszczególnych komórek."]},{"cell_type":"code","metadata":{"id":"B7AKvafo22N1"},"source":["pip install gspread_formatting"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDxANf45-FsS"},"source":["#### **Importujemy wymagane biblioteki**\n","\n","Część z poniżych bilibotek została już opisana w poprzednim przykładzie. Opiszemy tylko te, które nie były wspomniane:\n","*   **gspread_formatting** - dla wygody importujemy wszystkie elementy biblioteki i niektóre z nich wykorzystamy do zaimplementowania interfejsu w *Google Sheets*.\n","*   **time** - importujemy tę bibliotekę, aby skorzystać z funkcji *sleep()* w celu utworzenia interwałów."]},{"cell_type":"code","metadata":{"id":"6AEMykQ13PmP"},"source":["from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.model_selection import train_test_split\n","from oauth2client.client import GoogleCredentials\n","from modAL.models import ActiveLearner\n","from gspread_formatting import *\n","from google.colab import auth\n","from time import sleep\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import gspread"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZrNQZLL-_HQ"},"source":["#### **Uwierzytelnanie**\n","\n","Uwierzytelniamy się wprowadzając w wyznaczone pole tekstowe **token**, który wygenerowany zostanie dla nas za pomocą funkcji **.authenticate_user()**. Dzięki temu nawiążemy kontakt z naszym **Dyskiem Google**, który wykorzystamy w następnych krokach."]},{"cell_type":"code","metadata":{"id":"FJAHHhDQlTI5"},"source":["auth.authenticate_user()\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJry6ZNZlYW9"},"source":["#### **Odczytanie zawartości pliku**\n","Za pomocą poniższego kodu odczytujemy całą zawartość pliku **Mall_Customers**. Wykorzystujemy funkcję **.open()**, aby otworzyć wspomniany plik. Następnie poszczególne tablice danych przypisujemy do zmiennych, których nazwy odpowiadają nazwom powyżej opisanych kolumn.\n","\n","W zmiennej **parameters** zostały umieszczone kolumny, które wykorzystamy w trenowaniu algorytmu. Podobnie jak w poprzednim przykładzie, za pomocą **StandardScaler()** dokonujemy wstępnego przetworzenia danych zawartych w **parameters** i zapisujemy je w zmiennej **X**. Zmienna **y** przechowuje prawidłowe wartości **Spending Score** dla poszczególnych danych **X**.\n","\n","W przypadku cechy **GENRE**, wczytywane dane tekstowe **Male** i **Famale** zamieniamy kolejno na dane liczbowe **1** oraz **0**."]},{"cell_type":"code","metadata":{"id":"znfFzEUxjeBH"},"source":["worksheet = gc.open('Mall_Customers').sheet1\n","\n","GENRE = [float(1) if i.value == 'Male' else float(0) for i in worksheet.range('B2:B201')]\n","AGE = [float(i.value) for i in worksheet.range('C2:C201')]\n","ANNUAL_INCOME = [float(i.value) for i in worksheet.range('D2:D201')]\n","SPENDING_SCORE = [float(i.value) for i in worksheet.range('E2:E201')]\n","\n","parameters = np.array([GENRE, AGE, ANNUAL_INCOME])\n","\n","X = pd.DataFrame(parameters, index=['GENRE', 'AGE', 'ANNUAL_INCOME']).T\n","y = SPENDING_SCORE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVP-G9SLHtkH"},"source":["#### **Zobrazowanie danych**\n","Wykorzystując bibliotekę **pyplot** wyświetlimy poniżej, pobrane z **Google Sheets** dane, w postaci wykresów. Pierwsza komórka z kodem podlicza liczbę **mężczyzn** oraz **kobiet** w tablicy **GENRE**, druga podlicza liczbę osób w **danych przedziałach wiekowych** w tablicy **AGE**, a trzecia podlicza liczbę osób w **danych przedziałach dochodu rocznego**. Wszystkie z nich wyświetlają następnie dane w formie **wykresów słupkowych**.\n","\n","Funkcja **plt.bar** przyjmuje jako argumenty: **nazwy kolumn** oraz **wartości tych kolumn**."]},{"cell_type":"markdown","metadata":{"id":"aVFoFBg8La11"},"source":["##### **Płeć**"]},{"cell_type":"code","metadata":{"id":"hjnHAvKnEyyf"},"source":["male = GENRE.count(1)\n","female = GENRE.count(0)\n","plt.bar(['Male', 'Female'], [male, female])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ak_s191QLid3"},"source":["##### **Przedziały wiekowe**"]},{"cell_type":"code","metadata":{"id":"Qv6VTWCyF0fQ"},"source":["yo18_25 = 0\n","yo26_35 = 0\n","yo36_45 = 0\n","yo46_55 = 0\n","yo56_65 = 0\n","yo66_more = 0\n","\n","for i in AGE:\n","  if i >=18 and i <= 25:\n","    yo18_25 += 1\n","  elif i >=26 and i <= 35:\n","    yo26_35 += 1\n","  elif i >=36 and i <= 45:\n","    yo36_45 += 1\n","  elif i >=46 and i <= 55:\n","    yo46_55 += 1\n","  elif i >=56 and i <= 65:\n","    yo56_65 += 1\n","  else:\n","    yo66_more += 1\n","\n","ages = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']\n","plt.bar(ages, [yo18_25, yo26_35, yo36_45, yo46_55, yo56_65, yo66_more])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-avOaWWOUwi"},"source":["##### **Przedziały dochodu rocznego**"]},{"cell_type":"code","metadata":{"id":"cGt-fBrkMANj"},"source":["i0_20 = 0\n","i21_40 = 0\n","i41_60 = 0\n","i61_80 = 0\n","i81_100 = 0\n","i100_more = 0\n","\n","for i in ANNUAL_INCOME:\n","  if i >=0 and i <= 20:\n","    i0_20 += 1\n","  elif i >=21 and i <= 40:\n","    i21_40 += 1\n","  elif i >=41 and i <= 60:\n","    i41_60 += 1\n","  elif i >=61 and i <= 80:\n","    i61_80 += 1\n","  elif i >=81 and i <= 100:\n","    i81_100 += 1\n","  else:\n","    i100_more += 1\n","\n","ages = ['0-20', '21-40', '41-60', '61-80', '81-100', '100+']\n","plt.bar(ages, [i0_20, i21_40, i41_60, i61_80, i81_100, i100_more])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77daS4fFvCTj"},"source":["#### **Definiowanie regresora**\n","\n","Tworzymy obiekt **ActiveLearner**, któremu przypisujemy estymator **GaussianProcessRegressor()** i ustawiamy własną, zdefiniowaną strategię zapytań **GP_regression_std()**. W tym przypadku również nie uwzględniamy danych początkowych **X_training** i **y_training**."]},{"cell_type":"code","metadata":{"id":"KO95qMB_6D02"},"source":["def GP_regression_std(regressor, X):\n","    _, std = regressor.predict(X, return_std=True)\n","    query_idx = np.argmax(std)\n","    return query_idx, X.iloc[query_idx].to_frame().T\n","\n","kernel = DotProduct() + WhiteKernel()\n","\n","regressor = ActiveLearner(\n","    estimator=GaussianProcessRegressor(kernel=kernel),\n","    query_strategy=GP_regression_std\n",")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdpGanWFxMuu"},"source":["#### **Funkcja ucząca**\n","Funkcja przyjmuje cztery argumenty:\n","*   **query_inst** - wierz w zbiorze *X_train*, który został wybrany przez algorytm, jako ten najbardziej niepewny element w rankingu predykcji.\n","*   **answear** - wartość przypisana przez użytkownika dla danego wiersza *query_instance*.\n","*   **current_index** - aktualnie obserwowany wiersz w *Arkuszu Google*.\n","\n","Funkcja sprawdza jaki przedział **Spending Score** wybrał użytkownik, na podstawie argumentu **answear**. Wyróżniamy pięć przedziałów: **0-20** (dla **answear=0**), **21-40** (dla **answear=1**), **41-60** (dla **answear=2**), **61-80** (dla **answear=3**), **81-100** (dla **answear=4**). W zależności od wybranego przedziału zmiennej **answear** przypisujemy średnią obliczoną na podstawie minimalnej i maksymalnej wartości tego przedziału (np. dla przedziału **0-20** średnia równa jest **10** i taką wartość przypisujemy zmiennej **answear**). Korzystając z funkcji **.teach()** uczymy algorytm, przypisując przykładowi znajdującemu się w zmiennej **query_inst**, wartość **answear** wybraną przez użytkownika. Na koniec funkcja **.update_cell()** wprowadza do arkusza na **dziesiatej** kolumnie, w wierszu o indeksie **current_index+2** aktualny poziom wyuczenia algorytmu obliczony na bazie funkcji **.score()** i danych testowych."]},{"cell_type":"code","metadata":{"id":"A7RZ7_vG6KC2"},"source":["def learning(query_inst, answear, current_index):\n","  answear = answear[0][0]\n","  if answear == 0:\n","    answear = 10\n","  elif answear == 1:\n","    answear = 30.5\n","  elif answear == 2:\n","    answear = 50.5\n","  elif answear == 3:\n","    answear = 70.5\n","  else:\n","    answear = 90.5\n","  regressor.teach(query_inst, [answear])\n","  worksheet.update_cell(current_index+2, 10, str(round(regressor.score(X_test,y_test), 2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rN06hMHP7JWo"},"source":["#### **Funkcja rysująca checkboxy**\n","Funkcja odpowiedzialna za wyrysowanie pięciu **checkboxów**, z których każdy przypisany jest do jednego z przedziałów, dla każdej iteracji uczenia algorytmu, w kolejnych wierszach o indeksach **current_index+1** i w kolumnach w przedziale **<4 ; 9>**. Użytkownik będzie korzystał z wyrysowanych **checkboxów** w celu wybrania odpowiedniego według niego przedziału **Spending Score** dla podanych danych. Funkcja **.batch_update()** wysyła do **Google Sheeta** żądanie **request**, które zawiera parametry opisujące **checkboxy**. "]},{"cell_type":"code","metadata":{"id":"AuEsLs9awcV-"},"source":["def draw_checkboxes(current_index):\n","  sheetId = worksheet._properties['sheetId']\n","  requests = {\"requests\": [\n","      {\n","          \"repeatCell\": {\n","              \"cell\": {\"dataValidation\": {\"condition\": {\"type\": \"BOOLEAN\"}}},\n","              \"range\": {\"sheetId\": sheetId, \"startRowIndex\": int(current_index+1), \"endRowIndex\": int(current_index+2), \"startColumnIndex\": 4, \"endColumnIndex\": 9},\n","              \"fields\": \"dataValidation\"\n","          }\n","      },\n","  ]}\n","  doc.batch_update(requests)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GsfyAqrh84KJ"},"source":["#### **Funkcja odzaznaczająca wykonany wiersz**\n","Funkcja ta zostanie wykorzystana do pokolorowania na **zielono** wiersza, w którym użytkownik wybrał jeden z przedziałów. W **cellFormat()** ustalamy jaki kolor ma zostać nadany dla tła, jaki kolor ma przyjąć tekst oraz sposób ułożenia tekstu w komórce w arkuszu. Następnie korzystamy z funkcji **format_cell_range()**, aby zdefiniowane style w **fmt** przypisać do wybranych komórek, których lokalizację podajemy w drugim argumencie tej funkcji. Przedział kolumn jest zawsze ten sam **A:J**, zmienia się jedynie numer wiersza, który jest podawany przez wartość **current_index+2**."]},{"cell_type":"code","metadata":{"id":"-JMuwLlx7E6v"},"source":["def highlight_done_row():\n","  global current_index\n","  fmt = cellFormat(\n","    backgroundColor=color(0, 1, 0),\n","    textFormat=textFormat(bold=True, foregroundColor=color(1, 1, 1)),\n","    horizontalAlignment='CENTER'\n","  )\n","  format_cell_range(worksheet, f'A{current_index+2}:J{current_index+2}', fmt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ozAFfBXsIpfI"},"source":["#### **Pętla obserwująca zmiany w Arkuszu Google**\n","Poniższy kod rozpoczynamy od przypisania opisanych zmiennych:\n","*   **doc** - przechowuje dokument *Learning*, który został otwarty za pomocą funkcji *.open()*.\n","*   **worksheet** - przechowuje pierwszy arkusz dokumentu zapisanego w zmiennej *doc*. \n","*   **current_index** - przyjmuje wartość indeksu aktualnie obserwowanego wiersza.\n","*   **iteration** - przyjmuje wartość aktualnej iteracji uczenia aktywnego.\n","\n","Funkcja **.query()** wyznacza nam ze zbioru **X_train** próbkę, która znajduje się najwyższej w rankingu predykcji. Następnie w kolejnych liniach korzystamy z tej próbki i za pomocą funkcji **.update_cell()** ustawiamy w **trzecim** wierszu i **<1 ; 4>** kolumnach jej wartości (tj. **płeć**, **wiek** i **roczny dochód klienta**). W przypadku cechy **GENDER** wartości liczbowe **1** oraz **0** zamieniamy na **Male** oraz **Famale**, aby były one bardziej czytelne w arkuszu dla użytkownika. Kolejny krok polega na wywołaniu funkcji **draw_checkboxes()**, która została opisana wcześniej.\n","\n","Następnym elementem kodu jest pętra **while**. Tutaj w jej pierwszej linii czytamy wartości kolumn **E:I** z wiersza o indeksie **current_index+2**. Są to pozycje w arkuszu, na których zostały rozmieszczone **checkboxy**. W pętli **for** za pomocą **ifa** sprawdzamy, czy na którejś z tych pozycji nie wystąpiła wartość **TRUE**. Jeżeli tak, to wykonujemy zawartość **ifa**, w którym skorzystamy ze wcześniej opisanych funkcji. W pierwszej kolejności wywołujemy funkcje **learning()** oraz **highlight_done_row()**, wykonujemy inkrementację zmiennych **current_index** (w celu przeskoczenia na następny wiersz arkusza) oraz **iteration** i wywołujemy funkcję **draw_checkboxes()**. Kolejny etap pętli jest analogiczny do tego co działo się przed nią, a więc wykonujemy funkcję **.query()**, aby wyznaczyć ze zbioru **X_train** kolejną próbkę, która znajduje się najwyższej w rankingu predykcji i używając funkcji **.update_cell()** wypisujemy jej dane w odpowiednim wierszu i kolumnach.\n","\n","Aby ograniczyć liczbę **requestów** do arkusza, na końcu pętli **while** użyta została funkcja **sleep()**, dzięki czemu komórki w arkuszu są sprawdzane i aktualizowane raz na **trzy** sekundy."]},{"cell_type":"code","metadata":{"id":"00zghD_7Ghac"},"source":["doc = gc.open('Learning')\n","worksheet = doc.sheet1\n","current_index = 1\n","iteration = 1\n","\n","query_idx, query_inst = regressor.query(X_train)\n","\n","worksheet.update_cell(3, 1, str(iteration))\n","worksheet.update_cell(3, 2, 'Male' if X_train.iloc[query_idx]['GENRE'] == 1 else 'Female')\n","worksheet.update_cell(3, 3, X_train.iloc[query_idx]['AGE'])\n","worksheet.update_cell(3, 4, X_train.iloc[query_idx]['ANNUAL_INCOME'])\n","\n","draw_checkboxes(current_index)\n","\n","while True:\n","  CRIM = np.array([i.value for i in worksheet.range(f'E{str(current_index+2)}:I{str(current_index+2)}')])\n","  for i in CRIM:\n","    if i =='TRUE':\n","      learning(query_inst, np.where(CRIM==i), current_index)\n","      highlight_done_row()\n","      current_index += 1\n","      iteration += 1\n","      draw_checkboxes(current_index)\n","      \n","      query_idx, query_inst = regressor.query(X_train)\n","      \n","      worksheet.update_cell(current_index+2, 1, str(iteration))\n","      worksheet.update_cell(current_index+2, 2, 'Male' if X_train.iloc[query_idx]['GENRE'] == 1 else 'Female')\n","      worksheet.update_cell(current_index+2, 3, X_train.iloc[query_idx]['AGE'])\n","      worksheet.update_cell(current_index+2, 4, X_train.iloc[query_idx]['ANNUAL_INCOME'])\n","\n","  sleep(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZSSX-9i8qWo"},"source":["#### **Przykład wykonanego uczenia**\n","Na poniższym obrazku widzimy wykonanych osiem iteracji przez użytkownika, które doprowadziły do uzyskania przez algorytm wartości **score** równej **-0.01**. W każdej iteracji, użytkownik wybierał dla podanych danych jeden z **checkboxów**, określający przedział **Spending Score**, który według niego był najbardziej trafny.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1fVxOb2mEYIa9lP4dBRlOmLhd5dlwWQ2j)"]},{"cell_type":"markdown","metadata":{"id":"pZsdkd8RHHP1"},"source":["#### **Ograniczenia kwantyzacji danych**\n","Nie jesteśmy w stanie zejść poniżej pewnej wartości **score**, a wynika to z ograniczeń naszego interfejsu w **Google Sheets**, w którym danych nie wprowadzamy w postaci dokładnych wartości, lecz zostały one skwantyzowane do poziomu **przedziałów**. W ten sposób ograniczyliśmy liczbę rodzajów etykiet do **pięciu** wartości uzyskanych z obliczenia średniej na bazie najniższej i najwyższej wartości w danym przedziale. Dzięki temu nie musimy ograniczać puli użytkowników, etykietujących dla nas dane, do grupki ekspertów, którzy byliby w stanie podać nam dokładne liczby. Użytkownicy mają w tym wypadku więcej swobody, co przekłada się na bardziej skalowany proces etykietowania danych, ale jednocześnie wpływa negatywnie na jego dokładność."]},{"cell_type":"markdown","metadata":{"id":"LxVcpZGk_V2J"},"source":["### 8. Criteo Sponsored Search Conversion Log Dataset.\n","\n","W tym punkie skorzystamy z popularnego zbioru danych **Criteo Sponsored Search Conversion Log Dataset**. Zestaw ten zawiera logi uzyskane z **Criteo Predictive Search (CPS)**. **CPS** oferuje zautomatyzowane, kompleksowe rozwiązania wykorzystujące zaawansowane techniki uczenia maszynowego, które poprawiają komfort korzystania z **Google Shopping** dzięki niezawodnej, predykcyjnej optymalizacji we wszystkich aspektach kampanii reklamodawcy. Każdy wiersz w zbiorze danych reprezentuje kliknięcie wykonane przez użytkownika w reklamę związaną z produktem i zawiera informacje o **cechach produktu**, **czasie kliknięcia**, **cechach użytkownika** i **informacji o urządzeniu**. **Dataset** zawiera również informacje o tym, czy kliknięcia ostatecznie doprowadziły do ​transakcji w ciągu **30 dni** oraz o **czasie** między kliknięciem a transakcją.\n","\n","Baza ta waży domyślnie około **6.4GB** i można ją pobrać z tej strony:\n","\n","https://ailab.criteo.com/criteo-sponsored-search-conversion-log-dataset/\n","\n","My skorzystamy z odchudzonej wersji przygotowanej przez Pana dr inż. Andrzeja Szwabe, składającej się z **24.080** wierszy i ważącej około **2.5MB**. Możemy ją pobrać z poniższej strony i umieścić w **rootowym** folderze na naszym **Dysku Google**, tak samo jak w poprzednich przykładach. Pobieranie nie jest jednak konieczne, ponieważ możemy się połączyć bezpośrednio z **Arkuszem Google**, podając odpowiednie dane. Zostanie to przedstawione w jednym z następnych podpunktów. \n","\n","https://docs.google.com/spreadsheets/d/1_PNPgD9gfckqJDMPH7Rcn8wyY5zHrQKE9I1KoLrZbZE/edit?usp=sharing\n","\n","Kod, z którego będziemy korzystać, znajdujący się poniżej i został w zdecydowanej większości napisany przez Pana dr inż. Andrzeja Szwabe. Został on rozwinięty o dodatkowe funkcje i wprowadzono w nim drobne modyfikacje. "]},{"cell_type":"markdown","metadata":{"id":"RBuCBZ_uE_m7"},"source":["####**Instalacja biblioteki xgboost**\n","Z biblioteki **xgboost** wykorzystamy jeden z jej elementów, czyli **XGBRegressor**, który posłuży nam jako **estymator** poczas aktywnej regresji danych (podobnie jak w poprzednich przykładach korzystaliśmy z **GaussianProcessRegressor**). **XGBoost** to algorytmem uczenia maszynowego bazujący na drzewach, który korzysta ze struktury **wzmacniającej gradient**. Jest to idealne połączenie technik optymalizacji sprzętu i oprogramowania w celu osiągnięcia doskonałych wyników poprzez wykorzystanie minimalnych zasobów obliczeniowych w krótkim czasie.\n"]},{"cell_type":"code","metadata":{"id":"aUGISYZwo1MM"},"source":["pip install xgboost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbO9GR9jLkvN"},"source":["#### **Importujemy wymagane biblioteki**\n","\n","Część z poniżych bilibotek została już opisana w poprzednim przykładzie. Opiszemy tylko te, które nie były wspomniane:\n","*   **sklearn.metrics** - importujemy funkcje *mean_squared_error()* oraz *mean_absolute_error()*, które pozwolą nam obliczyć *Średni Błąd Kwadratowy* oraz *Średni Błąd Bezwzględny* modelu predykcyjnego.\n","*   **sklearn.preprocessing** - importujemy transformator *OneHotEncoder*, który koduj cechy kategoryczne na postać jednej tablicy liczbowej. Wejście do tego transformatora powinno być tablicą *liczb całkowitych* lub tablicą *stringów*, oznaczających wartości przyjmowane przez cechy kategoryczne, które są kodowane przy użyciu schematu kodowania *one-hot*. Kodowanie to jest potrzebne do dostarczenia danych kategorycznych do estymatora *XGBRegressor*.\n","*   **xgboost.sklearn** - importujemy estymator *XGBRegressor*, który wykorzystamy w procesie aktywnego uczenia.\n","*   **itertools** - importujemy funkcję *combinations*, która zwraca kombinacje danych wejściowych o określonej długości.\n","*   **random** - importujemy funkcję *sample()*, która pozwala wybrać losowy element ze zbioru.\n","*   **math** - importujemy tę bibliotekę, aby skorzystać z funkcji *sqrt()* w obliczenia pierwiastka kwadratowego."]},{"cell_type":"code","metadata":{"id":"JBJKs1gQLssO"},"source":["from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.preprocessing import OneHotEncoder\n","from xgboost.sklearn import XGBRegressor\n","from itertools import combinations\n","from random import sample\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0f8c5_Z8tb8"},"source":["####**Namiary na Arkusz Google**\n","W trzech poniżej wymienionych zmiennych umieszczamy kolejno identyfikator dokumentu znajdującego się na **Dysku Google** (**sheet_id**), nazwę arkusza w tym dokumencie (**sheet_name**) oraz pełny **url** wraz z wprowadzonymi wcześniej danymi w odpowiednich miejscach. Dane zapisujemy w ten sposób, aby później wygodnie móc się do nich odnosić w dalszej części kodu."]},{"cell_type":"code","metadata":{"id":"lVlO8dTooyT3"},"source":["sheet_id = \"1_PNPgD9gfckqJDMPH7Rcn8wyY5zHrQKE9I1KoLrZbZE\"\n","sheet_name = \"CriteoSearchData_partner_C0F515F0A2D0A5D9F854008BA76EB537\"\n","url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IrmZjV9OxK4S"},"source":["#### **Pobieranie danych**\n","Poniżej zostały przedstawione trzy funkcje, które tworzą pewien ciąg logiczny, gdzie jedna korzysta z drugiej, a ostatecznym wynikiem ich działania jest zbiór danych pochodzący z zawartego w zmiennej **url** **Arkusza Google**, okrojonego o pewne kolumny.\n","\n","Główną funkcją, będącą rodzicem, jest funkcja **make_X_and_y()**. Zwraca ona dwie tablice, z których jedna stanowi zbiór danych wejściowych produktu (**X**), a druga prawdziwe wartości wyjściowe (**y**).\n","\n","Funkcja **read_CPSD_per_partner_data_group_CSV_file()** odpowiada za pobranie danych z **Arkusza Google** i dostarczenie ich w formie **DataFrame** do funkcji **make_product_df()**. W niej z kolei pozbywamy się wierszy zawierajacych produkty, których cena równa jest **zero** oraz uszczuplamy dane, pozostawiając wyłacznie kolumny, które są zawarte w tablicy **useful_columns**. Wynik jej działania trafia do funkcji **make_X_and_y()**, gdzie dane określające cechy produktu poddane zostają transformacji za pomocą **OneHotEncoder**. Na zakończenie funkcja zwraca gotowe do użycia podczas uczenia oraz predykcji dane **X** i **y**.\n","\n","Wracając do funkcji **make_product_df()**, zwraca ona **DataFrame** z następującymi kolumnami (tylko one nas będą interesowały podczas uczenia i predykcji):\n","*   **product_price** - cena produktu pokazana w ogłoszeniu.\n","*   **product_age_group** - docelowa grupa wiekowa, dla której produkt jest przeznaczony.\n","*   **product_gender** - docelowa płeć, dla której produkt jest przeznaczony.\n","*   **product_brand** - marka produktu.\n","*   **product_category (1-7)** - cechy kategoryczne związane z produktem. Nie jest ujawnione znaczenia tych cech.\n","\n"]},{"cell_type":"code","metadata":{"id":"kyX34xwDn_h2"},"source":["pd.options.mode.chained_assignment = None\n","\n","def read_CPSD_per_partner_data_group_CSV_file(nrows=None):\n","    dtypes = {'Sale': 'int64', 'SalesAmountInEuro': 'float64', 'time_delay_for_conversion': 'int64',\n","                   'click_timestamp': 'int64', 'nb_clicks_1week': 'int64', 'product_price': 'float64',\n","                   'product_age_group': 'string', 'device_type': 'string', 'audience_id': 'string',\n","                   'product_gender': 'string', 'product_brand': 'string', '<product_category_1>': 'string',\n","                   '<product_category_2>': 'string', '<product_category_3>': 'string', '<product_category_4>': 'string',\n","                   '<product_category_5>': 'string', '<product_category_6>': 'string', '<product_category_7>': 'string',\n","                   'product_country': 'string', 'product_id': 'string', 'product_title': 'string',\n","                   'partner_id': 'string', 'user_id': 'string'}\n","    return pd.read_csv(url, nrows=nrows, dtype=dtypes)\n","\n","def make_product_df():\n","    CPSD_per_partner_data_df = read_CPSD_per_partner_data_group_CSV_file()\n","    useful_rows_mask = CPSD_per_partner_data_df[\"product_price\"] != 0.0\n","    temp_product_df = CPSD_per_partner_data_df[useful_rows_mask]\n","    useful_columns = [\"product_price\", \"product_age_group\", \"product_gender\", \"product_brand\"]\n","    additional_useful_columns = []\n","    for raw_column in list(CPSD_per_partner_data_df):\n","        if raw_column.find(\"<product_category_\") != -1:\n","              additional_useful_columns.append(raw_column)\n","    useful_columns += additional_useful_columns\n","    return temp_product_df[useful_columns]\n","\n","def make_X_and_y():\n","    product_df = make_product_df()\n","    list_of_column_names = list(product_df)\n","    list_of_feature_names = list(set(list_of_column_names).difference(set([\"product_price\"])))\n","    X_df = product_df[list_of_feature_names].astype(\"str\")\n","    y_series = product_df[\"product_price\"]\n","    X_array = X_df.to_numpy()\n","    X_array = np.nan_to_num(X_array)\n","    OH_encoder = OneHotEncoder(sparse=False)\n","    OH_encoder.fit(X_array)\n","    OH_encoded_X_dense_matrix = OH_encoder.transform(X_array)\n","    return OH_encoded_X_dense_matrix, np.array(y_series)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5guvpyILZfYV"},"source":["#### **Klasa IndiciesBootstrapper**\n","Z klasy **IndiciesBootstrapper** będziemy korzystać w klasie **AlternativeSamplesBootstrapper**, która została opisana w następnym podpunkcie. Bieżąca klasa składa się z **konstruktora** oraz **trzech metod bootstrapingu**:\n","*   **bootstraps_diversity_first_indices_list**\n","*   **classical_independent_bootstraps_indices_list**\n","*   **enhanced_classical_independent_bootstraps_indices_list**\n","\n","**Bootstraping** w skrócie polega na równomiernym rozdzieleniu zbioru na daną liczbę podzbiorów składających się z różnych elementów, w losowo dobranej kolejności. Istnieją różne metody wynonywania tej operacji.\n","\n","Każda z trzech wymienionych metod, przyjmuje jako argument tablicę **indices_list**. Składa się ona z chronologicznie ułożonych liczb w przedziale **<0 ; n>**, gdzie **n** w naszym przypadku to długość zbioru danych wejściowych **X**. Po wprowadzeniu takiego ciągu do jednej z wymienionych funkcji, jako wyjście otrzymujemy pewną liczbę tablic (którą zapisaliśmy w **self.number_of_bootstraps**) składających się z losowo dobranych wartości z tego ciagu."]},{"cell_type":"code","metadata":{"id":"0NOgAsRW79z2"},"source":["class IndiciesBootstrapper:\n","\n","    def __init__(self, number_of_bootstraps, min_bootstrap_size):\n","        self.number_of_bootstraps = number_of_bootstraps\n","        self.min_bootstrap_size = min_bootstrap_size\n","        self.bootstraps = []\n","        for i in range(number_of_bootstraps):\n","            self.bootstraps.append([])\n","\n","    def bootstraps_diversity_first_indices_list(self, indices_list):\n","        indices_list_size = len(indices_list)\n","        bootstrap_size = indices_list_size - 1\n","        temp_unique_combinations = [comb for comb in combinations(indices_list, bootstrap_size)]\n","        temp_unique_combinations = np.array(temp_unique_combinations)\n","        np.random.shuffle(temp_unique_combinations)\n","        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n","            self.bootstraps[temp_bootstrap_idx] = temp_unique_combinations[temp_bootstrap_idx].tolist()\n","        return self.bootstraps\n","\n","    def classical_independent_bootstraps_indices_list(self, indices_list):\n","        indices_list_size = len(indices_list)\n","        potential_bootstrap_size = int(indices_list_size/self.number_of_bootstraps)\n","        bootstrap_size = max(potential_bootstrap_size, self.min_bootstrap_size)\n","        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n","            self.bootstraps[temp_bootstrap_idx] = list(np.random.choice(indices_list, bootstrap_size, replace=False))\n","        return self.bootstraps\n","\n","    def enhanced_classical_independent_bootstraps_indices_list(self, indices_list):\n","        indices_list_size = len(indices_list)\n","        no_repetitions_bootstrap_size = int(indices_list_size/self.number_of_bootstraps)\n","        bootstrap_size = max(no_repetitions_bootstrap_size, self.min_bootstrap_size)\n","        additional_samples_size = bootstrap_size - no_repetitions_bootstrap_size\n","        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n","            self.bootstraps[temp_bootstrap_idx] = list(np.random.choice(indices_list, no_repetitions_bootstrap_size, replace=False))\n","            self.bootstraps[temp_bootstrap_idx] += list(np.random.choice(indices_list, additional_samples_size, replace=False))\n","        return self.bootstraps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYbKTxObZkoV"},"source":["#### **Klasa AlternativeSamplesBootstrapper**\n","Z klasy **AlternativeSamplesBootstrapper** będziemy korzystać w klasie **AlternativeCommitteeRegressor**, która została opisana w następnym podpunkcie. Bieżąca klasa składa się z **konstruktora** oraz funkcji **make_bootstrap()**. \n","\n","W konstruktorze jako argumenty podajemy **liczbę bootstrapów** oraz **minimalny rozmiar pojedynczego bootstrapa**, a w jego ciele deklarujemy obiekt **self.bootstraps**, w którym będziemy przechowywać dane **X** i **y** oraz obiekt klasy **IndiciesBootstrapper**, któremu przekazujemy zawarte w nagłówku konstruktora dane.\n","\n","Jeżeli chodzi o funkcję **make_bootstraps()** to przyjmuje ona jako argumenty zbiór danych wejściowych **X** oraz zbiór danych wyjściowych **y**. W samym jej ciele, za pośrednictwem obiektu klasy **IndiciesBootstrapper** wywołujemy funkcję **.bootstraps_diversity_first_indices_list()** (czyli jedną z metod **bootstrapingu**), której przekazujemy rozmiar naszego zbioru danych **X** i w efekcie otrzymujemy wcześniej zadeklarowaną liczbę **bootstrapów**, którą wykorzystujemy do przetasowania danych **X** i **y**, które następnie umieszczamy w tablicach **self.bootstraps[\"X\"]** oraz **self.bootstraps[\"y\"]**. Jako efekt wykonania funkcji, otrzymujemy obiekt **self.bootstraps**.\n","\n","Jeżeli masz ochotę, możesz przetestować pozostałe dwie metody **bootstrapingu** podmieniając funkcję **bootstraps_diversity_first_indices_list()**."]},{"cell_type":"code","metadata":{"id":"zUIM-dfKiHG7"},"source":["class AlternativeSamplesBootstrapper:\n","\n","    def __init__(self, number_of_bootstraps, min_bootstrap_size):\n","        self.number_of_bootstraps = number_of_bootstraps\n","        self.min_bootstrap_size = min_bootstrap_size\n","        self.bootstraps = {\"X\":[], \"y\":[]}\n","        self.indicies_bootstrapper = IndiciesBootstrapper(self.number_of_bootstraps, self.min_bootstrap_size)\n","\n","    def make_bootstraps(self, X, y):\n","        indices_list = list(range(len(X)))\n","        self.indices_bootstraps = self.indicies_bootstrapper.bootstraps_diversity_first_indices_list(indices_list)\n","        X_as_array = np.array(X)\n","        y_as_array = np.array(y)\n","        self.bootstraps = {\"X\":[], \"y\":[]}\n","        for temp_bootstrap_idx in range(self.number_of_bootstraps):\n","            self.bootstraps[\"X\"].append(X_as_array[self.indices_bootstraps[temp_bootstrap_idx], :])\n","            self.bootstraps[\"y\"].append(y_as_array[self.indices_bootstraps[temp_bootstrap_idx]])\n","        return self.bootstraps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62PyFGU4Zp5k"},"source":["#### **Klasa AlternativeCommitteeRegressor**\n","Ostatnią klasą, z której będziemy korzystać jest klasa **AlternativeCommitteeRegressor**. Jest ona zbudowana z **konstruktora** oraz **sześciu metod**, które przypominają te pochodzące z biblioteki **modAL**.\n","\n","Metodę, którą zastosujemy do wyznaczania miary niepewności danych zaproponował w 2005 roku **Gilad-Bachrach**, która polega na wykorzystaniu sporu komitetu regresorów, bazującego na średniej ich wyników. Stanowi ona bardziej efektywną i optymalną obliczeniowo alternatywę dla **GaussianProcessRegressor**. W konstruktorze jako argumenty podajemy **rodzaj** regresora jaki chcemy zastosować oraz **liczbę** tych regresorów.\n","\n","Poniżej opiszemy metody, z których klasa korzysta:\n","*   **teach()** - jako agrumenty podajemy jej dane wejściowe *X* oraz dane wyjściowe *y*, które są dodawane do tablic: *self.X* oraz *self.y*. Metoda ta odpowiada za dostarczenie nowej wartości uczącej dla obiektu klasy.\n","*   **_make_bootstraps()** - jako argumenty podajemy zestaw danych wejściowych *X* oraz zestaw danych wyjściowych *y*, na bazie których tworzymy zadeklarowaną liczbę *bootstrapów*.\n","*   **_committee_train()** - za pomocą tej metody szkolimy wszystkie regresory, które zostały wprowadzone do tablicy *self.committee*, podczas tworzenia obiektu.\n","*   **predict()** - korzystając z tej metody, zapisujemy w tablicy *self.committee_predictions* przewidywane wartości *y* dla danych wejściowych *X_query*, dostarczonych przez argument metody.\n","*   **_compute_means_and_STDs_for_query()** - metoda ta zapisuje do zmiennych *self._committee_means* oraz *self._committee_STDs* kolejno *średnią wartość* oraz *odchylenie standardowe* każdej trójelementowej grupy pól z tablic predykcyjnych wartości dostarczonych przez każdy z regresorów w komitecie, na podstawie danych wejściowych *X_query* przekazanych w argumencie.\n","*   **query()** - jako argument tej metody przekazujemy zbiór danych wejściowych *X_query*. Funkcja ta na podstawie obliczonej wcześniej tablicy odchyleń standardowych *self._committee_STDs*, znajduje element, którego wartość odchylenia jest największa i zwraca indeks tego elementu."]},{"cell_type":"code","metadata":{"id":"5uQRuUL0niz0"},"source":["class AlternativeCommitteeRegressor:\n","\n","    def __init__(self, variance_unaware_regressor_class, number_of_regressors_in_commitee):\n","        self.number_of_regressors_in_committee = number_of_regressors_in_commitee\n","        self.min_bootstrap_size = 2\n","        self.committee = []\n","        for regressor_idx in range(self.number_of_regressors_in_committee):\n","            self.committee.append(variance_unaware_regressor_class(silent=True))\n","        self.X = []\n","        self.y = []\n","        self.samples_bootstraper = AlternativeSamplesBootstrapper(self.number_of_regressors_in_committee, self.min_bootstrap_size)\n","\n","    def teach(self, X, y):\n","        self.X.append(X)\n","        self.y.append(y)\n","\n","    def _make_bootstraps(self):\n","        self.bootstraps = self.samples_bootstraper.make_bootstraps(self.X, self.y)\n","\n","    def _committee_train(self):\n","        for regressor_idx in range(self.number_of_regressors_in_committee):\n","            self.committee[regressor_idx].fit(X=self.bootstraps[\"X\"][regressor_idx], y=self.bootstraps[\"y\"][regressor_idx])\n","\n","    def predict(self, X_query):\n","        self.committee_predictions = []\n","        for regressor_idx in range(self.number_of_regressors_in_committee):\n","            self.committee_predictions.append(self.committee[regressor_idx].predict(X_query))\n","\n","    def _compute_means_and_STDs_for_query(self, X_query):\n","        self._make_bootstraps()\n","        self._committee_train()\n","        self.predict(X_query)\n","        self.committee_predictions_array = np.array(self.committee_predictions)\n","        self._committee_means = np.mean(self.committee_predictions_array, axis=0)\n","        self._committee_STDs = np.std(self.committee_predictions_array, axis=0)\n","\n","    def query(self, X_query):\n","        self._compute_means_and_STDs_for_query(X_query)\n","        self.query_result = np.argmax(self._committee_STDs)\n","        return self.query_result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d49c-n2Tr6E1"},"source":["#### **Miary efektywności modelu**\n","\n","Aby ustalić poziom efektywności naszego regresora na przestrzeni kolejnych iteracji jego działania, skorzystamy z dwóch poniższych miar wydajności.\n","\n","**Średni błąd kwadratowy (MSE - Mean Squared Error)** - stosunek sumy kwadratów odchyleń poszczególnych pomiarów od wartości średniej policzonej wzorem przez liczbę wszystkich pomiarów. Im mniejszy jest współczynnik **MSE** dla modelu, tym model jest lepszy.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1mKiA9behRwex1d8N-xE8HiTpa4seZrO-)\n","\n","**Średni błąd bezwzględny (MAE - Mean Absolute Error)** - odchylenie od wartości rzeczywistej. Informuje jakim błędem miarowym jest obarczona nasza prognoza.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1eRk-LQIufSTWaTODPO5tN7Hygq259jfA)"]},{"cell_type":"markdown","metadata":{"id":"FFNKihQ-CNJM"},"source":["#### **Uczenie modelu za pomocą AlternativeCommitteeRegressor**\n","W tym podpunkcie wytrenujemy nasz model na dostarczonych danych z **Criteo Sponsored Search Conversion Log Dataset** za pomocą naszego autorskiego regresora **AlternativeCommitteeRegressor**. Poniższy blok kodu jest na tyle obszerny, że łatwiej będzie go opisywać za pomocą umieszczonych w nim komentarzy. Przy okazji zbieżemy statystyki dotyczące zmian efektywności algorytmu."]},{"cell_type":"code","metadata":{"id":"Rlz3kaImbWgS"},"source":["# Pobieramy dane i rozmieszczamy je losowo.\n","X, y = make_X_and_y()\n","shuffled_indices = np.arange(y.shape[0])\n","np.random.shuffle(shuffled_indices)\n","y = y[shuffled_indices]\n","X = X[shuffled_indices]\n","\n","# Ustalamy liczbę regresorów oraz początkowych danych treningowych i tworzymy obiekt klasy AlternativeCommitteeRegressor.\n","number_of_regressors_in_commitee = 2\n","number_of_initial_samples = 2 * number_of_regressors_in_commitee\n","committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n","\n","# Wyznaczamy tablicę danych początkowych.\n","initial_X_samples = X[:number_of_initial_samples]\n","initial_y_samples = y[:number_of_initial_samples]\n","\n","# Wyznaczamy tablicę danych treningowych.\n","X_query = X[number_of_initial_samples:]\n","y_query = y[number_of_initial_samples:]\n","\n","# Wprowadzamy dane początkowe do obiektu i za pomocą funkcji query(), w tym przypadku nie uzyskujemy indeksu przykładu o największej wartości uczącej, lecz wprowadzamy te dane do modelu, ucząc każdy z jego regresorów. \n","for temp_initial_sample_idx in range(number_of_initial_samples):\n","  X_train_sample = initial_X_samples[temp_initial_sample_idx]\n","  y_train_sample = initial_y_samples[temp_initial_sample_idx]\n","  committee.teach(X_train_sample, y_train_sample)\n","committee.query(X_query)\n","\n","# Tworzymy tablice, w których zapisywane będą na bieżąco: Średnie Błędy Kwadratowe oraz Średnie Błędy Bezwzględne.\n","MSE_scores_of_AlternativeCommitteeRegressor = []\n","MAE_scores_of_AlternativeCommitteeRegressor = []\n","\n","# Wartość odpowiadająca za zmienną w czasie intensywność eksploracji (przez randomizację).\n","annealing_factor = 0.04\n","\n","# Tworzymy pętle działającą na zasadzie query -> learn.\n","for i in range(200):\n","  # Znajdujemy indeks przykładu o największej wartości uczącej dla algorytmu.\n","  query_result = committee.query(X_query)\n","\n","  # Wykonujemy randomizację sterującą intensywnością eksploracji w czasie.\n","  temp_prob = np.log(np.random.rand())\n","  temp_prob = temp_prob / (annealing_factor * i + 1)\n","  temp_prob = int(round(temp_prob)) % 2\n","\n","  # Jeżeli temp_prob jest większe od 0.5 to query_result wybieramy w sposób losowy.\n","  if temp_prob > 0.5:\n","      query_result = sample(list(range(X_query.shape[0])), 1)[0]\n","  \n","  # Obliczamy Średni Błąd Kwadratowy oraz Średni Błąd Bezwzględny i umieszamy je w dedykowanych tablicach.\n","  temp_score_MSE = mean_squared_error(y_query, committee._committee_means)\n","  MSE_scores_of_AlternativeCommitteeRegressor.append(temp_score_MSE)\n","  temp_score_MAE = mean_absolute_error(y_query, committee._committee_means)\n","  MAE_scores_of_AlternativeCommitteeRegressor.append(temp_score_MAE)\n","\n","  # Wprowadzamy dane wejściowe X oraz dane wyjściowe y o indeksie query_result do modelu.\n","  committee.teach(X_query[query_result], y_query[query_result])\n","\n","  # Pozbywamy się z tablic X_query oraz y_query elementu o indeksie query_result.\n","  X_query = X_query[np.arange(len(X_query))!=query_result]\n","  y_query = y_query[np.arange(len(y_query))!=query_result]\n","\n","  # Co dziesiątą iterację wyświetlamy akturalny Średni Błąd Kwadratowy\n","  if i % 10 == 0 or i == 199:\n","    print(f'MSE ( {i} iterations): {temp_score_MSE}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kSbXEfwsW1Nk"},"source":["#### **Uczenie modelu za pomocą losowo dobranych danych**\n","Aby zdobyć pewien pogląd na temat efektywności uczenia algorytmu na bazie naszego regresora, porównamy przebieg jego uczenia zapisany w tablicach **MSE_scores_of_AlternativeCommitteeRegressor** oraz **MAE_scores_of_AlternativeCommitteeRegressor** z przebiegiem opartym wyłącznie na danych dobieranych losowo. W tym celu, kolejny raz wytrenujemy nasz model za pomocą poniższego kodu. Kod ponownie zostanie opisany za pomocą umieszczonych w nim komentarzy."]},{"cell_type":"code","metadata":{"id":"bABCBnzxKDqT"},"source":["# Wyznaczamy liczbę początkowych danych treningowych i tworzymy obiekt klasy AlternativeCommitteeRegressor.\n","number_of_initial_samples = 2 * number_of_regressors_in_commitee\n","committee = AlternativeCommitteeRegressor(XGBRegressor, number_of_regressors_in_commitee)\n","\n","# Wyznaczamy tablicę danych początkowych.\n","initial_X_samples = X[:number_of_initial_samples]\n","initial_y_samples = y[:number_of_initial_samples]\n","\n","# Wyznaczamy tablicę danych treningowych.\n","X_query = X[number_of_initial_samples:]\n","y_query = y[number_of_initial_samples:]\n","\n","# Wprowadzamy dane początkowe do obiektu i za pomocą funkcji query() przekazujemy je dalej do modelu, ucząc każdy z jego regresorów. \n","for temp_initial_sample_idx in range(number_of_initial_samples):\n","    X_train_sample = initial_X_samples[temp_initial_sample_idx]\n","    y_train_sample = initial_y_samples[temp_initial_sample_idx]\n","    committee.teach(X_train_sample, y_train_sample)\n","committee.query(X_query)\n","\n","# Tworzymy tablice, w których zapisywane będą na bieżąco: Średnie Błędy Kwadratowe oraz Średnie Błędy Bezwzględne.\n","MSE_scores_of_random_baseline = []\n","MAE_scores_of_random_baseline = []\n","\n","for i in range(200):\n","    # query_result wybieramy w sposób losowy.\n","    query_result = committee.query(X_query)\n","    query_result = sample(list(range(X_query.shape[0])), 1)[0]\n","\n","    # Obliczamy Średni Błąd Kwadratowy oraz Średni Błąd Bezwzględny i umieszamy je w dedykowanych tablicach.\n","    temp_score_MSE = mean_squared_error(y_query, committee._committee_means)\n","    MSE_scores_of_random_baseline.append(temp_score_MSE)\n","    temp_score_MAE = mean_absolute_error(y_query, committee._committee_means)\n","    MAE_scores_of_random_baseline.append(temp_score_MAE)\n","\n","    # Wprowadzamy dane wejściowe X oraz dane wyjściowe y o indeksie query_result do modelu.\n","    committee.teach(X_query[query_result], y_query[query_result])\n","\n","    # Pozbywamy się z tablic X_query oraz y_query elementu o indeksie query_result.\n","    X_query = X_query[np.arange(len(X_query))!=query_result]\n","    y_query = y_query[np.arange(len(y_query))!=query_result]\n","\n","    # Co dziesiątą iterację wyświetlamy akturalny Średni Błąd Kwadratowy\n","    if i % 10 == 0 or i == 199:\n","      print(f'MSE ( {i} iterations): {temp_score_MSE}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbqS1M_vdrCs"},"source":["#### **Porównanie przebiegów**\n","Za pomocą dwóch poniższych bloków kodu, w których korzystamy z biblioteki **pyplot**, możemy zobrazować dane opisujące **Średni Błąd Kwadratowy (MSE)** oraz **Średni Błąd Bezwzględny (MAE)** pochodzące z każdej iteracji uczenia algorytmu z wykorzystaniem **AlternativeCommitteeRegressor** oraz **danych wybranych losowo**.\n","\n","Jak widzimy po wykresach, linia niebieska przez znaczną ilość czasu znajduje się pod linią pomarańczową, co oznacza że algorytm korztstający z **AlternativeCommitteeRegressor** jest lepszym wyborem niż **dane dobierane losowo**."]},{"cell_type":"code","metadata":{"id":"CT_YLxf3d3dE"},"source":["plt.plot(MSE_scores_of_AlternativeCommitteeRegressor, label=\"AlternativeCommitteeRegressor\")\n","plt.plot(MSE_scores_of_random_baseline, label=\"Random\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBiiqbY_d49n"},"source":["plt.clf()\n","plt.plot(MAE_scores_of_AlternativeCommitteeRegressor, label=\"AlternativeCommitteeRegressor\")\n","plt.plot(MAE_scores_of_random_baseline, label=\"Random\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-d8Q_j3LdxO"},"source":["### Bibliografia\n","\n","*   https://ichi.pro/pl/wprowadzenie-do-aktywnego-uczenia-sie-19232985241548, 23.10.2021\n","*   https://www.youtube.com/watch?v=W2bJH0iXTKc&ab_channel=PyData, Jan Freyberg, 18.11.2019\n","*   http://proceedings.mlr.press/v70/bachman17a/bachman17a.pdf, Philip Bachman, Alessandro Sordoni, Adam Trischler\n","*   https://towardsdatascience.com/the-exploration-exploitation-dilemma-f5622fbe1e82, Joseph Rocca, 18.04.2021\n","*   http://www.vincentlemaire-labs.fr/publis/ijcnn_2_2010_camera_ready.pdf, A. Bondu, V. Lemaire, M. Boullé, 2010\n","*   https://modal-python.readthedocs.io/en/latest/, 06.11.2021\n","*   https://colab.research.google.com/notebooks/snippets/sheets.ipynb#scrollTo=6d0xJz3VzLOo, 12.11.2021\n","*   https://colab.research.google.com/notebooks/io.ipynb#scrollTo=WgXqE02UofZG, 12.11.2021\n","*   https://colab.research.google.com/drive/1emADQSjzHMilbqno2lcKbiAn4ewJ94DB, dr inż. Andrzej Szwabe, 26.11.2021\n","*   https://datascience.eu/pl/programowanie-komputerowe/xgboost/, Data Science Team, 15.05.2020\n","*   https://visualmonsters.cba.pl/prognozowanie/blad-e-blad-procentowy-ep-sredni-blad-me-sredni-procentowy-blad-mpe-sredni-blad-bezwzgledny-mae-sredni-bezwzgledny-blad-procentowy-mape/, 28.11.2021\n","*   https://pl.wikibooks.org/wiki/Statystyka_matematyczna/B%C5%82%C4%99dy_pomiarowe_w_fizyce, 28.11.2021\n","\n","\n","\n"]}]}